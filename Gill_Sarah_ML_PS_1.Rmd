---
title: "Gill_Sarah_ML_PS_1"
author: "Sarah Gill"
date: "1/13/2020"
output: pdf_document
---
# Problem Set 1: Learning and Regression
## Introduction to Machine Learning

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Desktop/Machine Learning/PS_1")
library(readr)
library(tidyverse)
```


## Statistical and Machine Learning

Supervised learning is a means of producing predictive models given data where the features and labels are known. The learner is able to improve (reduce the error), over iterations by responding to feedback. Learning is conceptualized as the machine’s response to this feedback (it’s error rate). Thus the data, with both X and Y, is used to iteratively refine the algorithm or mapping process of X to Y as the machine tries to minimize the error. This is much like regression analysis if we were looking only for the best model of the data regardless of theory. The target of supervised learning is to fit a model well to the dataset (given X and Y) such that it accurately predicts the Y for data outside of the dataset. Initially this means doing well in the test portion of the data (kept out from the initial training dataset), but in the end the idea is to be able predict Ys that are not known. For instance, supervised machine learning is often used in financial forecasting. 

Unsupervised learning is a means of discovering patterns in data, either directly or through dimension-reduction. With unsupervised learning the dataset only has the features (X), and not the labels (Y). The learning is not supervised in that the machine is not given the feedback of how well it did in each iteration, as is the case in supervised learning, but instead engages in pattern discovery.  The number of classes does not need to be known. The goal is not to create a predictive model, but rather to learn about the data. For example, unsupervised learning has been used to generate possible protean folding combinations for researchers to investigate in a bio-chemistry research lab. 



```{r cars}
names(mtcars)
summary(mtcars)
```
## 1. Linear Regression Regression
### a. Predict miles per gallon (mpg) as a function of cylinders (cyl). What is the output and parameter values for your model?
```{r 1a}
# Here we regress wage on IQ and then on age, bivariate 
#lm(wage ~ IQ, data = wage)

model1 <- lm(mpg ~ cyl, data = mtcars)
summary(model1)
#predict(model1)

```
There is a statistically significant relationship between number of cylinders and mpg.
Each additional cylinder is associated with a decrease in miles per gallon of 2.876mpg. 
These results show a theoretical mpg of 37.88mpg for the impossible car that has no cylinders. 



### b. Write the statistical form of the simple model in the previous question (i.e., what is the population regression function?).

mpg = intercept + b*cyl + error


### c. Add vehicle weight (wt) to the specification. Report the results and talk about differences in coefficient size, effects, etc.
```{r 1c}
model2 <- lm(mpg ~ cyl + wt, data = mtcars)
summary(model2)
#predict(model2)

```

There is a statistically significant relationship between number of cylinders and mpg and between weight and mpg. Each additional cylinder is associated with 1.508 fewer miles per gallon, holding weight constant. Each additional 1000lbs (one unit in wt) is associated with 3.191 fewer miles per gallon, holding number of cylinders constant. 

These results show a theoretical mpg of 39.69mpg for the impossible car that weighs nothing and has no cylinders. 

Note that the magnitude of the estimate for cyl is less than in the univariate regression. Some of the decrease in mpg that was associated with cly in the previous model may be better explained by weight.



### d. Interact weight and cylinders and report the results. What is the same or different? What are we theoretically asserting by including a multiplicative interaction term in the function?

```{r 1d}
model3 <- lm(mpg ~ cyl*wt, data = mtcars) 
summary(model3)
#predict(model3)
```
When we include an interaction term for wt and cyl, the coefficient estimates for both wt and cyl increase in magnitude (they are both more negative). However these coefficients can no longer be interpreted on their own. By including an interaction term we are making the assertion that the relationship between weight and number of cylinders moderates or mediates the relationship between weight and mpg and between number of cylinders and mpg. In other words, we assert that the relationship between cylinders and mpg is effected by weight and the relationship between weight and mpg is effected by number of cylinders.
This assertion is supported by the results, with the estimate for the interaction term being statistically significant, and it's inclusion improving our Adjusted R-squared.
The estimated relationship is:


cyl on mpg:  -3.81 + 0.81*wt
wt on mpg:  -8.66 + 0.81*cyl

So for a car that is the mean weight in this dataset, an increase of one additional cylinder is associated with a decrease of roughly 1.20mpg.

mean wt in dataset: 3.217 

```{r}
 -3.8032 + 0.8084 *(3.217)
```
Or for a car that has the mean number of cylinders for this dataset, an increase of an additional 1000lbs (one unit of wt) is associated with a decrease of roughly 3.65mpg

mean cyl in dataset: 6.188
```{r}
-8.6556  + 0.8084 *(6.188)
```





## 2. Non-linear Regression
### a. Fit a polynomial regression, predicting wage as a function of a second order polynomial for age. Report the results and discuss the output
```{r 2a}
wage_data <- read_csv("wage_data.csv")

wage_model <- lm(wage ~ age + I(age^2), data = wage_data)
#wage_model <- lm(wage ~ poly(age, degree = 2, raw = T), data = wage_data)
summary(wage_model)
```
### c. Describe the output. What do you see substantively? What are we asserting by fitting a polynomial regression?

By fitting a polynomial to this data, we are asserting that the relationship between age and wage may be different at different ages. Using a quadratic function we are allowing for the relationship to be concave or convex and even to change signs.

Here we see that at lower ages increasing age is associated with increasing wage, however this is increasing at a decreasing rate (as indicated by the negative sign on the age^2 coefficient, thus our fitted curve is concave). Also the positive slope may change to negative at some age. 

increasing age by one year is associated with a change in wage of 5.29 - 0.053(age)

### b. Plot the function with 95% confidence interval bounds.
```{r, warning=FALSE}
# plot
#works but throws an error
y = wage_data$wage
x = wage_data$age
plot(x,y,col=rgb(0.4,0.4,0.8,0.6),pch=16 , cex=1.3, xlab = "Wage", ylab = "Age") 

#myPredict <- predict( wage_model ) 
#ix <- sort(x,index.return=T)$ix
#lines(x[ix], myPredict[ix], col=2, lwd=2 ) 

#Curve
myPredict <- predict( wage_model , interval="predict")
ix <- sort(x,index.return=T)$ix
lines(x[ix], myPredict[ix , 1], col=2, lwd=2 )

CI <- predict(wage_model, x=wage, interval = 'confidence', level=0.95)
#ix <- sort(x,index.return=T)$ix
#lines(x[ix], CI[ix , 1], col=2, lwd=2 )
#CI
polygon(c(rev(x[ix]), x[ix]), c(rev(CI[ ix,3]), CI[ ix,2]), col = rgb(0.7,0.7,0.7,0.4) , border = "black", alpha = 1)

#cite: https://www.r-graph-gallery.com/44-polynomial-curve-fitting.html
#cite: https://www.r-graph-gallery.com/45-confidence-interval-around-polynomial-curve-fitting.html
#cite: https://www.researchgate.net/post/How_can_I_put_confidence_intervals_in_R_plot
```
From the plot we can see that wage increases with age until about 50, when each additional year of age is then associated with a decrease in wage. 



### d. How does a polynomial regression differ both statistically and substantively from a linear regression
```{r 2d}
lin_wage_model = lm(wage ~ age, data = wage_data)

anova(lin_wage_model, wage_model)
#cite: https://www.youtube.com/watch?v=ZYN0YD7UfK4
```
The polynomial regression allows for a non-linear relationship between age and wage and thus is a more flexibal though harder to interpret modle.
The model that includes age squared is statistically significantly better at predicting age than the purely linear model. 


